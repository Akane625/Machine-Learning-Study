{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc3775d-f302-4c19-afd4-43f2fe40354d",
   "metadata": {},
   "source": [
    "# The Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that summarizes the performance of a classification model by comparing **actual values** vs. **predicted values**.\n",
    "\n",
    "|               | Predicted: 0 | Predicted: 1 |\n",
    "|---------------|--------------|--------------|\n",
    "| **Actual: 0** | TN           | FP           |\n",
    "| **Actual: 1** | FN           | TP           |\n",
    "\n",
    "*The figure above displays a confusion matrix for binary classification*\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Parts of a Confusion Matrix\n",
    "\n",
    "| Term | Also Known As | Definition | Description |\n",
    "|------|---------------|------------|-------------|\n",
    "| **True Positive (TP)** | Hits | âœ… Actual = 1, Predicted = 1 | Data says YES, model says YES (Correct) |\n",
    "| **True Negative (TN)** | Correct Rejections | âœ… Actual = 0, Predicted = 0 | Data says NO, model says NO (Correct) |\n",
    "| **False Positive (FP)** | False Alarm / Type I Error | âŒ Actual = 0, Predicted = 1 | Data says NO, model says YES (Wrong) |\n",
    "| **False Negative (FN)** | Miss / Type II Error | âŒ Actual = 1, Predicted = 0 | Data says YES, model says NO (Wrong) |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Simple Memory Aid:\n",
    "\n",
    "| Cell | Question it Answers | Correct? |\n",
    "|------|---------------------|----------|\n",
    "| **TP** | Did we correctly identify what IS there? | âœ… Yes |\n",
    "| **TN** | Did we correctly identify what ISN'T there? | âœ… Yes |\n",
    "| **FP** | Did we see something that WASN'T there? | âŒ No (Type I) |\n",
    "| **FN** | Did we miss something that WAS there? | âŒ No (Type II) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64060aed-8983-4872-9eb5-774dc074ef5c",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "\n",
    "**Accuracy** is the overall proportion of correct predictions made by the model out of all predictions made. It is represented by the formula below:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $TP$ = True Positives (correctly predicted positives)\n",
    "- $TN$ = True Negatives (correctly predicted negatives)\n",
    "- $FP$ = False Positives (Type I Errors)\n",
    "- $FN$ = False Negatives (Type II Errors)\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Given the confusion matrix:\n",
    "\n",
    "|               | Predicted: 0 | Predicted: 1 |\n",
    "|---------------|--------------|--------------|\n",
    "| **Actual: 0** | TN = 70      | FP = 5       |\n",
    "| **Actual: 1** | FN = 10      | TP = 15      |\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Accuracy} &= \\frac{15 + 70}{15 + 70 + 5 + 10} \\\\\n",
    "&= \\frac{85}{100} \\\\\n",
    "&= 0.85 \\text{ or } 85\\%\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Intuitive Understanding\n",
    "\n",
    "| Component | What it Represents |\n",
    "|-----------|-------------------|\n",
    "| **Numerator** (TP + TN) | âœ… **All correct predictions** (both positives and negatives) |\n",
    "| **Denominator** (TP + TN + FP + FN) | ðŸ“Š **All predictions made** (the entire dataset) |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… When Accuracy is a Good Metric\n",
    "\n",
    "Accuracy works well when:\n",
    "- **Balanced classes** (similar number of 0s and 1s)\n",
    "- **Equal error costs** (FP and FN are equally costly)\n",
    "- **Simple benchmark** needed for model comparison\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ When High Accuracy is Bad\n",
    "\n",
    "### 1. Class Imbalance\n",
    "\n",
    "**Scenario:** Fraud Detection (only 1% of transactions are fraudulent)\n",
    "\n",
    "A model that predicts \"NOT FRAUD\" for EVERY transaction:\n",
    "\n",
    "|               | Predicted: No | Predicted: Yes |\n",
    "|---------------|---------------|----------------|\n",
    "| **Actual: No**    | TN = 9,900    | FP = 0         |\n",
    "| **Actual: Yes**   | FN = 100      | TP = 0         |\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{9,900 + 0}{9,900 + 0 + 0 + 100} = \\frac{9,900}{10,000} = 99\\%\n",
    "$$\n",
    "\n",
    "**The Problem:** This model catches **ZERO** fraudulent transactions\n",
    "\n",
    "**Real-world examples:**\n",
    "- Disease screening (rare conditions)\n",
    "- Fraud detection\n",
    "- Churn prediction\n",
    "- Manufacturing defects\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Unequal Error Costs\n",
    "\n",
    "| Scenario | Cost of FP | Cost of FN |\n",
    "|----------|------------|------------|\n",
    "| Cancer Screening | Patient anxiety | Death |\n",
    "| Spam Detection | Lost email | Inbox clutter |\n",
    "| Self-driving Car | Unnecessary brake | Accident |\n",
    "\n",
    "Standard Accuracy treats all errors equally:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{Total}\n",
    "$$\n",
    "\n",
    "But in reality:\n",
    "\n",
    "$$\n",
    "\\text{Total Cost} = (FP \\times \\text{Cost}_{FP}) + (FN \\times \\text{Cost}_{FN})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The Accuracy Paradox\n",
    "\n",
    "**Dataset:** 95% Class 0, 5% Class 1\n",
    "\n",
    "**Strategy A** (Always predict 0):\n",
    "$$\n",
    "\\text{Accuracy} = 95\\%\n",
    "$$\n",
    "\n",
    "**Strategy B** (Try to find Class 1):\n",
    "Even at 80% recall on Class 1:\n",
    "$$\n",
    "\\text{Accuracy} = (0.95 \\times 1.0 + 0.05 \\times 0.8) = 95\\%\n",
    "$$\n",
    "\n",
    "The model gets punished for trying to find the minority class\n",
    "\n",
    "---\n",
    "\n",
    "### 4. When You Care About One Class\n",
    "\n",
    "| If you care about... | Accuracy hides... |\n",
    "|---------------------|-------------------|\n",
    "| Finding ALL positives (high recall) | How many positives were missed (FN) |\n",
    "| Being sure when you predict positive (high precision) | How many false alarms (FP) |\n",
    "| Rare event detection | Complete failure on the rare class |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” The Detection Test\n",
    "\n",
    "Ask these questions to know if high accuracy is lying to you:\n",
    "\n",
    "- [ ] Is my data balanced? (Check class distribution)\n",
    "- [ ] Are error costs equal? (FP cost = FN cost?)\n",
    "- [ ] Do I care about both classes equally?\n",
    "- [ ] Did I check the confusion matrix?\n",
    "- [ ] Did I calculate per-class metrics (precision/recall)?\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š What to Do Instead\n",
    "\n",
    "### Always Report These Alongside Accuracy:\n",
    "\n",
    "- **Confusion Matrix** - See exactly where errors occur\n",
    "- **Precision & Recall** - Understand performance per class\n",
    "- **F1 Score** - Harmonic mean of precision & recall\n",
    "- **ROC-AUC** - Performance across all thresholds\n",
    "- **Class-wise Accuracy** - Accuracy on Class 0 vs Class 1\n",
    "\n",
    "---\n",
    "\n",
    "### Better Metrics for Imbalanced Data:\n",
    "\n",
    "| Metric | Formula | When to Use |\n",
    "|--------|---------|-------------|\n",
    "| Balanced Accuracy | $\\frac{1}{2}\\left(\\frac{TP}{P} + \\frac{TN}{N}\\right)$ | Imbalanced classes |\n",
    "| F1 Score | $2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$ | When you care about positives |\n",
    "| Precision-Recall AUC | Area under PR curve | Rare event detection |\n",
    "| Cohen's Kappa | $\\frac{p_o - p_e}{1 - p_e}$ | Chance-corrected agreement |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary Checklist\n",
    "\n",
    "**High accuracy is BAD when:**\n",
    "\n",
    "â–¡ Dataset is imbalanced (>90% one class)  \n",
    "â–¡ False negatives are expensive  \n",
    "â–¡ False positives are expensive  \n",
    "â–¡ You only care about one class  \n",
    "â–¡ Model always predicts majority class  \n",
    "â–¡ Confusion matrix shows poor minority class performance  \n",
    "â–¡ Business metrics disagree with accuracy  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ The Golden Rule\n",
    "\n",
    "> **\"Accuracy tells you how often you're right, but not about what, when, or at what cost.\"**\n",
    "\n",
    "Always dig deeper than accuracy alone\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Key Takeaway\n",
    "\n",
    "> **Accuracy tells you how often you're right overall, but not *what kind* of mistakes you're making.**\n",
    "\n",
    "Always pair accuracy with:\n",
    "- Confusion matrix inspection\n",
    "- Precision & recall\n",
    "- Domain knowledge about error costs\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Quick Reference\n",
    "\n",
    "```python\n",
    "# Formula\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# Interpretation\n",
    "if accuracy is high:\n",
    "    print(\"Model gets most predictions right\")\n",
    "else:\n",
    "    print(\"Model struggles to predict correctly\")\n",
    "    \n",
    "# But remember...\n",
    "if high_accuracy and imbalanced_data:\n",
    "    print(\"ðŸš© RED FLAG ðŸš© - Investigate further!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b97fdd-a2b4-48a9-b40b-ddbd70ce7f91",
   "metadata": {},
   "source": [
    "# Precision\n",
    "\n",
    "**Precision** answers the question: *\"Of all the positive predictions we made, how many were actually correct?\"*\n",
    "\n",
    "It focuses on the **trustworthiness** of positive predictions.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $TP$ = True Positives (correctly predicted positives)\n",
    "- $FP$ = False Positives (Type I Errors)\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Given the confusion matrix:\n",
    "\n",
    "|               | Predicted: 0 | Predicted: 1 |\n",
    "|---------------|--------------|--------------|\n",
    "| **Actual: 0** | TN = 70      | FP = 5       |\n",
    "| **Actual: 1** | FN = 10      | TP = 15      |\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Precision} &= \\frac{15}{15 + 5} \\\\\n",
    "&= \\frac{15}{20} \\\\\n",
    "&= 0.75 \\text{ or } 75\\%\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Interpretation:** When the model predicts positive, it is correct 75% of the time.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuitive Understanding\n",
    "\n",
    "| Component | What it Represents |\n",
    "|-----------|-------------------|\n",
    "| **Numerator** (TP) | âœ… Correct positive predictions |\n",
    "| **Denominator** (TP + FP) | ðŸŽ¯ **All positive predictions made** |\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{Correct Positives}}{\\text{All Positives Predicted}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… When Precision is the Right Metric\n",
    "\n",
    "Precision is important when:\n",
    "- **False Positives are costly** (crying wolf is expensive)\n",
    "- You need to be **sure** before taking action\n",
    "- The cost of acting on a wrong positive is high\n",
    "\n",
    "### Real-world Examples:\n",
    "\n",
    "| Scenario | Why Precision Matters |\n",
    "|----------|----------------------|\n",
    "| **Spam Detection** | Marking real email as spam (FP) loses important messages |\n",
    "| **Drug Testing** | False positive could ruin someone's career |\n",
    "| **Quality Control** | Rejecting good products (FP) wastes money |\n",
    "| **Medical Testing** | False positive causes unnecessary anxiety and procedures |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Precision vs. Other Metrics\n",
    "\n",
    "| If you want to know... | Use this |\n",
    "|------------------------|----------|\n",
    "| \"Should I trust a positive prediction?\" | **Precision** |\n",
    "| \"Did I find all the positives?\" | Recall |\n",
    "| \"Overall, how am I doing?\" | Accuracy |\n",
    "| \"Balance between precision and recall?\" | F1 Score |\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ The Precision Trap\n",
    "\n",
    "High precision doesn't mean high recall:\n",
    "- Model that makes only ONE positive prediction (and gets it right):\n",
    "- Precision = 1/1 = 100% (perfect!)\n",
    "- Recall = 1/100 = 1% (terrible!)\n",
    "\n",
    "**Always pair precision with recall!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab7089-02a9-4e58-bd2e-94256d8ddb1f",
   "metadata": {},
   "source": [
    "# Recall\n",
    "\n",
    "**Recall** (also called **Sensitivity** or **True Positive Rate**) answers the question: *\"Of all the actual positives, how many did we catch?\"*\n",
    "\n",
    "It focuses on **completeness** of positive predictions.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $TP$ = True Positives (correctly predicted positives)\n",
    "- $FN$ = False Negatives (Type II Errors)\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Given the confusion matrix:\n",
    "\n",
    "|               | Predicted: 0 | Predicted: 1 |\n",
    "|---------------|--------------|--------------|\n",
    "| **Actual: 0** | TN = 70      | FP = 5       |\n",
    "| **Actual: 1** | FN = 10      | TP = 15      |\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Recall} &= \\frac{15}{15 + 10} \\\\\n",
    "&= \\frac{15}{25} \\\\\n",
    "&= 0.60 \\text{ or } 60\\%\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Interpretation:** The model caught 60% of all actual positives.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuitive Understanding\n",
    "\n",
    "| Component | What it Represents |\n",
    "|-----------|-------------------|\n",
    "| **Numerator** (TP) | âœ… Correct positive predictions |\n",
    "| **Denominator** (TP + FN) | ðŸŽ¯ **All actual positives in the data** |\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{Correct Positives}}{\\text{All Actual Positives}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… When Recall is the Right Metric\n",
    "\n",
    "Recall is important when:\n",
    "- **False Negatives are costly** (missing something is dangerous)\n",
    "- You need to **catch as many positives** as possible\n",
    "- The cost of missing a positive is high\n",
    "\n",
    "### Real-world Examples:\n",
    "\n",
    "| Scenario | Why Recall Matters |\n",
    "|----------|-------------------|\n",
    "| **Cancer Screening** | Missing a cancer patient (FN) could be fatal |\n",
    "| **Fraud Detection** | Missing a fraudulent transaction costs money |\n",
    "| **Terrorist Threat** | Missing a threat has catastrophic consequences |\n",
    "| **Machine Failure** | Missing warning signs causes breakdowns |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Recall vs. Other Metrics\n",
    "\n",
    "| If you want to know... | Use this |\n",
    "|------------------------|----------|\n",
    "| \"Did I catch all the positives?\" | **Recall** |\n",
    "| \"Should I trust a positive prediction?\" | Precision |\n",
    "| \"Overall, how am I doing?\" | Accuracy |\n",
    "| \"Balance between precision and recall?\" | F1 Score |\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ The Recall Trap\n",
    "\n",
    "High recall doesn't mean high precision:\n",
    "- Model that predicts EVERYTHING as positive:\n",
    "- Recall = 100/100 = 100% (perfect!)\n",
    "- Precision = 100/1000 = 10% (terrible!)\n",
    "\n",
    "**Always pair recall with precision!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d3944-86c2-4d14-b4e1-c01ed18ac176",
   "metadata": {},
   "source": [
    "# F1 Score\n",
    "\n",
    "**F1 Score** is the **harmonic mean** of Precision and Recall. It provides a single score that balances both concerns.\n",
    "\n",
    "---\n",
    "\n",
    "## Harmonic Mean Formula\n",
    "\n",
    "$$\n",
    "\\text{Harmonic Mean} = \\frac{n}{\\frac{1}{x_1} + \\frac{1}{x_2} + \\cdots + \\frac{1}{x_n}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Derivation of F1 Score\n",
    "\n",
    "### Step 1: The Harmonic Mean Formula\n",
    "\n",
    "The harmonic mean of two numbers $a$ and $b$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Harmonic Mean} = \\frac{2}{\\frac{1}{a} + \\frac{1}{b}}\n",
    "$$\n",
    "\n",
    "Unlike the arithmetic mean, the harmonic mean penalizes extreme values more severely.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Apply to Precision and Recall\n",
    "\n",
    "Let $P$ = Precision and $R$ = Recall. Substituting into the harmonic mean formula:\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2}{\\frac{1}{P} + \\frac{1}{R}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Simplify the Denominator\n",
    "\n",
    "Find a common denominator for $\\frac{1}{P} + \\frac{1}{R}$:\n",
    "\n",
    "$$\n",
    "\\frac{1}{P} + \\frac{1}{R} = \\frac{R}{P \\times R} + \\frac{P}{P \\times R} = \\frac{P + R}{P \\times R}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Substitute Back\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2}{\\frac{P + R}{P \\times R}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Simplify the Complex Fraction\n",
    "\n",
    "Dividing by a fraction is the same as multiplying by its reciprocal:\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\times \\frac{P \\times R}{P + R}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Final Formula\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Derivation Summary\n",
    "\n",
    "| Step | Operation | Result |\n",
    "|------|-----------|--------|\n",
    "| 1 | Start with harmonic mean formula | $F_1 = \\frac{2}{\\frac{1}{P} + \\frac{1}{R}}$ |\n",
    "| 2 | Find common denominator | $\\frac{1}{P} + \\frac{1}{R} = \\frac{P + R}{P \\times R}$ |\n",
    "| 3 | Substitute back | $F_1 = \\frac{2}{\\frac{P + R}{P \\times R}}$ |\n",
    "| 4 | Multiply by reciprocal | $F_1 = 2 \\times \\frac{P \\times R}{P + R}$ |\n",
    "\n",
    "---\n",
    "\n",
    "## Why Harmonic Mean?\n",
    "\n",
    "Harmonic mean penalizes extreme values more than arithmetic mean:\n",
    "\n",
    "| Model | Precision | Recall | Arithmetic Mean | **F1 Score (Harmonic)** |\n",
    "|-------|-----------|--------|-----------------|------------------------|\n",
    "| A | 100% | 100% | 100% | **100%** |\n",
    "| B | 100% | 0% | 50% | **0%** |\n",
    "| C | 80% | 80% | 80% | **80%** |\n",
    "\n",
    "**F1 is 0 if either precision or recall is 0** - it forces balance!\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Using our previous calculations:\n",
    "- Precision = 0.75 (75%)\n",
    "- Recall = 0.60 (60%)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F_1 &= 2 \\times \\frac{0.75 \\times 0.60}{0.75 + 0.60} \\\\\n",
    "&= 2 \\times \\frac{0.45}{1.35} \\\\\n",
    "&= 2 \\times 0.333 \\\\\n",
    "&= 0.667 \\text{ or } 66.7\\%\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Intuitive Understanding\n",
    "\n",
    "| Score | Meaning |\n",
    "|-------|---------|\n",
    "| **F1 = 1.0** | Perfect precision and recall |\n",
    "| **F1 > 0.8** | Strong balance between precision and recall |\n",
    "| **F1 ~ 0.5** | Mediocre performance |\n",
    "| **F1 = 0** | Either precision or recall is zero |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… When F1 is the Right Metric\n",
    "\n",
    "F1 is important when:\n",
    "- You need a **single number** to compare models\n",
    "- You care about **both precision and recall**\n",
    "- You have **imbalanced classes**\n",
    "- You want to **balance** false positives and false negatives\n",
    "\n",
    "---\n",
    "\n",
    "## The Precision-Recall Trade-off\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚   Perfect       â”‚\n",
    "                â”‚   Model         â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†‘\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚    F1 Score     â”‚\n",
    "                â”‚  (Balances both)â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â†™                 â†˜\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚Precisionâ”‚           â”‚ Recall  â”‚\n",
    "        â”‚  (Trust)â”‚           â”‚(Catch all)â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "\n",
    "**You can't always have both:**\n",
    "- Increasing threshold â†‘ Precision â†“ Recall\n",
    "- Decreasing threshold â†“ Precision â†‘ Recall\n",
    "\n",
    "---\n",
    "\n",
    "## F-Beta Score: Weighted Version\n",
    "\n",
    "For when you care more about one metric:\n",
    "\n",
    "$$\n",
    "F_\\beta = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{(\\beta^2 \\times \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "| $\\beta$ | Emphasis | When to Use |\n",
    "|---------|----------|-------------|\n",
    "| $\\beta = 1$ | Equal weight | F1 Score (balanced) |\n",
    "| $\\beta = 2$ | Recall matters more | Disease screening |\n",
    "| $\\beta = 0.5$ | Precision matters more | Spam detection |\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Comparison Table\n",
    "\n",
    "| Metric | Formula | Asks | Best For |\n",
    "|--------|---------|------|----------|\n",
    "| **Precision** | $\\frac{TP}{TP + FP}$ | \"Should I trust positive predictions?\" | When FP are costly |\n",
    "| **Recall** | $\\frac{TP}{TP + FN}$ | \"Did I catch all positives?\" | When FN are costly |\n",
    "| **F1 Score** | $2 \\times \\frac{P \\times R}{P + R}$ | \"Balance both?\" | Imbalanced data |\n",
    "| **Accuracy** | $\\frac{TP + TN}{TP + TN + FP + FN}$ | \"Overall correctness?\" | Balanced classes |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Checklist\n",
    "\n",
    "### Use Precision When:\n",
    "â–¡ False Positives are expensive  \n",
    "â–¡ You need certainty before acting  \n",
    "â–¡ The cost of being wrong is high  \n",
    "\n",
    "### Use Recall When:\n",
    "â–¡ False Negatives are expensive  \n",
    "â–¡ Missing a positive is dangerous  \n",
    "â–¡ You need to catch everything  \n",
    "\n",
    "### Use F1 When:\n",
    "â–¡ You need a balanced metric  \n",
    "â–¡ Classes are imbalanced  \n",
    "â–¡ You want one number for comparison  \n",
    "\n",
    "---\n",
    "\n",
    "## The Golden Rule\n",
    "\n",
    "> **\"Precision says 'trust me', Recall says 'I got you', F1 says 'let's be reasonable'.\"**\n",
    "\n",
    "Choose based on what failure costs more! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
