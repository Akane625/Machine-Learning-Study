# ðŸ‘‹ Welcome to My ML/AI Learning Repository

Hi there! ðŸŒŸ This repository is my personal **learning playground** where I document additional insights about Machine Learning and AI. Think of it as my digital notebook ðŸ““ where I experiment with curious topics, dig deeper into interesting concepts, and try to answer the "why" behind everything.

> ðŸ’¡ **Why this exists:** I want to learn more and understand the levels of ML/AI whilst answering my curious questions about some things related to them

---

## ðŸ§  Models I've Explored So Far

Here are the ML models I've learned about so far.

| ðŸ¤– Model | ðŸ“ Notes | âŒ¨ï¸ Type |
|----------|----------|------------------|
| **Linear Regression (OLS)** | Predicts continuous values using ordinary least squares (The basics) | Linear/Parametric |
| **Linear Probability Model** | Simple binary outcomes but has limitations | Linear/Parametric (Binary) |
| **Logistic Regression** |  Better than LPM - uses sigmoid function to keep probabilities between 0-1 | Linear/Parametric (Binary) |
| **Generalized Linear Model (Binomial)** | A specific family of GLM (this is the same as LOGIT) | Linear/Parametric (Binary) |
| **K-Nearest Neighbors** | Find the closest to predictor then vote | Instance-Based/Non-Parametric |
| **Decision Trees** | Like a flowchart for decisions | Tree-Based/Non-Parametric |
| **Bagging** | Bootstrap Aggregating - many models > one model | Ensemble (Tree-Based) |
| **Random Forest** | A forest of decision trees | Ensemble (Tree-Based) |
| **AdaBoost** | ... | ... |
| **Gradient Boost** | ... | ... |
| **XGBoost** | ... | ... |

---

## ðŸ“… Learning Schedule

Curious about what I'm studying next? Check out my schedule to see what topics I'm exploring!

| ðŸ“† Date | ðŸ“š Lesson | 
|:-------:|-----------|
| **Feb 27, 2026** | **Understanding Classification Metrics**<br>â€¢ Define each metric Accuracy, Precision, Recall, F1, ROC-AUC<br>â€¢ Explore their mathematical approaches |
| **Mar 2, 2026** | **F1 Dilema and Quick Review of Performance Metrics**<br>â€¢ What cases does higher or lower F1 is better<br>â€¢ Quickly review $R^2$, Log-Likelihood, AIC, BIC |
| **Mar 4, 2026** | **Model Analysis (LPM, LOGIT, KNN, DT)**<br>â€¢ Explore the math, logic, interpretations, and code of the following models: <br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Linear Probability Model<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Logistics Regression<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ K-Nearest Neighbours<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Decision Tree |
| **Mar 6, 2026** | **Model Analysis (Bagging, RF, AdaB, GradB, XGB)**<br>â€¢ Explore the math, logic, interpretations, and code of the following models: <br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Bagging<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Random Forest<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ AdaBoost<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Gradient Boost<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ XGBoost |
| **Mar 9, 2026** | **Model Comparison**<br>â€¢ Random Forest (with Grid Search) vs Bagging vs Decision Trees<br>â€¢ Use cross-validation |

Happy Learning! ðŸš€

*P.S. This is a living document - I'm constantly adding new things as I learn them. Check back every Mondays, Wednesdays, and Fridays!* âœ¨
